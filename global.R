source('dependencies.R')

q_round1 <- read_document(file = "part1.txt")
q_round2 <- read_document(file = "part2.txt")
class_combo <- c(q_round1, q_round2)

a <- 28 #count of people
b <- 6  #number of questions

my_df <- as.data.frame(matrix(nrow=a, ncol=b))

for(z in 1:b){
  for(i in 1:a){
    my_df[i,z]<- class_combo[i*b+z-b]
  }#closing z loop
}#closing i loop

### Tokenizing answer 1 -------------------

answer_1 <- my_df$V1 #1 question 
answer_1 <- substr(answer_1, start=11 , stop = 10000)

mydf1 <- data_frame(line=1:a, text=answer_1)

###custom stop words  and tokenizing
cust_stop <- data_frame(word=c("spend", "time"), lexicon=rep("cust", each=2))

data(stop_words)

frequencies_tokens_nostop_1 <- mydf1 %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  anti_join(cust_stop) %>%#here's where we remove tokens
  count(word, sort=TRUE)

### Tokenizing answer 2 -------------------

answer_2 <- my_df$V2 #2 question 
answer_2 <- substr(answer_2, start=11 , stop = 10000)

mydf2 <- data_frame(line=1:a, text=answer_2)

###custom stop words  and tokenizing
cust_stop_2 <- data_frame(word=c("socialize", "socializing"), lexicon=rep("cust", each=2))

data(stop_words)

frequencies_tokens_nostop_2 <- mydf2 %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>% 
  anti_join(cust_stop_2) %>% 
  count(word, sort=TRUE)

### Tokenizing answer 3 -------------------

answer_3 <- my_df$V3 #1 question 
answer_3 <- substr(answer_3, start=11 , stop = 10000)

mydf3 <- data_frame(line=1:a, text=answer_3)

###custom stop words  and tokenizing
cust_stop_3 <- data_frame(word=c("kids"), lexicon=rep("cust", each=1))

data(stop_words)

frequencies_tokens_nostop_3 <- mydf3 %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>% 
  anti_join(cust_stop_3) %>% 
  count(word, sort=TRUE)

### Tokenizing answer 4 -------------------

answer_4 <- my_df$V4 
answer_4 <- substr(answer_4, start=11 , stop = 10000)

mydf4 <- data_frame(line=1:a, text=answer_4)

cust_stop_4 <- data_frame(word=c("commute"), lexicon=rep("cust", each=1))

data(stop_words)

frequencies_tokens_nostop_4 <- mydf4 %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>% 
  anti_join(cust_stop_4) %>% 
  count(word, sort=TRUE)

### Tokenizing answer 5 -------------------

answer_5 <- my_df$V5 
answer_5 <- substr(answer_5, start=11 , stop = 10000)

mydf5 <- data_frame(line=1:a, text=answer_5)

cust_stop_4 <- data_frame(word=c("wear", "clothes"), lexicon=rep("cust", each=2))

data(stop_words)

frequencies_tokens_nostop_5 <- mydf5 %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>% 
  count(word, sort=TRUE)

### All together -------------------

all_df <- bind_rows(
  mutate(frequencies_tokens_nostop_1, question= "first"), 
  mutate(frequencies_tokens_nostop_2, question= "second"),
  mutate(frequencies_tokens_nostop_3, question= "third"),
  mutate(frequencies_tokens_nostop_4, question= "fourth"),
  mutate(frequencies_tokens_nostop_5, question= "fifth"))

all_df_freq <- all_df %>%  
  #count(sentiment, term, wt = count) %>%  
  ungroup() %>%  
  dplyr::top_n(8) %>%  
  #mutate(n = ifelse(sentiment == "negative", -n, n)) %>%  
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = n)) +  
  geom_bar(stat = "identity") +  ylab("Contribution to sentiment") +  
  coord_flip()


#Bi-gram ----------------

a <- length(class_combo)
b <- 1
bidf <- data_frame(line=1:a, text=class_combo)

my_bigrams <- bidf %>%
  unnest_tokens(bigram, text, token = "ngrams", n=2) %>% 
  dplyr::count(bigram, sort = TRUE) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  dplyr::count(word1, word2, sort = TRUE) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = 100), show.legend = FALSE)+
  geom_node_point(color = "lightblue", size= 3 )+
  geom_node_text(aes(label=name), vjust =1, hjust=1, repel = TRUE)+#, repel=TRUE)
  theme_void()

### LDA --------

my_dtm <- all_df %>%
  cast_dtm(question, word, n)

my_dtm_1 <- LDA(my_dtm, k = 2, control = list(seed = 123))

#now we are looking for the per topic per word probabilities aka. beta
#beta - what is the probability that "this term" will be generated by "this topic"

my_dtm_topics <- tidy(my_dtm_1, matrix = "beta")

name_topics <- c(`1` = "Dressing", `2` = "Commute")

top_terms_my_dtm <- my_dtm_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>% #heck if it works
  ungroup() %>%
  arrange(topic, -beta) %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~topic, scales = "free", labeller = labeller(topic = name_topics)) +
  coord_flip()

#lets calculate the relative difference between the betas for words in topic 1
#and words in topic 2

### Prediction model ----------

a <- 28
b <- 5
my_dfp <- data.frame(line=1:a, matrix(nrow=a, ncol=b))


for(z in 1:b){
  for(i in 1:a){
    my_dfp[i,z+1]<- class_combo[i*b+z-b]
  }#closing z loop
}#closing i loop

my_dfp$combined <- ''
for (i in 1:ncol(my_dfp)) {
  my_dfp$combined <- paste(my_dfp$combined, my_dfp[, i], sep=  ' ')
}


mydf_dtm <- my_dfp %>%
  unnest_tokens(word, combined) %>%
  anti_join(stop_words) %>% #here's where we remove tokens
  count(line, word, sort=TRUE) %>%
  cast_dtm(line, word, n)


#mydf_dtm
mydata <- tidy(mydf_dtm)

matrix_data <- mydata %>% 
  cast_dfm(document, term, count)

data.train <- matrix_data[1:25, c(-ncol(matrix_data))]
data.test <- matrix_data[26:28, c(-ncol(matrix_data))]
#building the Naive Bayes model:
lebels <- c(rep(1, 25), rep(0, 5))
#data.train
NB_classifier <- textmodel_nb(data.train, c(1,0,0,1,1,1,0,1,0,1,1,0,0,0,1,0,0,1,0,1,1,1,0,0,1))
#,1,0,1

# predicting the testing data
pred <- predict(NB_classifier, data.test)

### Function for uploaded files -----------

create_df <- function(p, q, up_df){
  my_df <- as.data.frame(matrix(nrow=p, ncol=q))
  
  for(z in 1:q){
    for(i in 1:p){
      my_df[i,z] <- up_df[i*q+z-q]
      my_df[i,z] <- substr(my_df[i,z], start=11 , stop = 10000)
    }
  }
  
  return(my_df)
}

token_function <- function(df, q_col){
  p_row = nrow(df)
    
  answer <- df[,q_col] #1 question 
  answer_clean <- substr(answer, start=11 , stop = 10000) #To eliminate the "You said"
  
  df1 <- data_frame(line = 1:p_row, text = answer_clean)
  
  ###custom stop words  and tokenizing
  #########  <------- We need to add an input so people can add their custom stop_words
  #cust_stop <- data_frame(word = c("spend", "time"), lexicon = rep("cust", each=2))
  
  data(stop_words)
  
  frequencies_tokens_nostop <- df1 %>%
    unnest_tokens(word, text) %>%
    anti_join(stop_words) %>%
    #anti_join(cust_stop) %>%#here's where we remove tokens
    dplyr::count(word, sort=TRUE)
  
  end <- frequencies_tokens_nostop %>%
    inner_join(get_sentiments("bing")) %>% # pull out only sentiment words
    dplyr::count(sentiment) %>% # count the # of positive & negative words
    spread(sentiment, n, fill = 0) %>% # made data wide rather than narrow
    mutate(net_sentiment = positive - negative)
  
  return(end)
}

bigram_network <- function(a, b, bidf){
  a <- length(class_combo)
  b <- 1
  bidf <- data_frame(line=1:a, text=class_combo)
  
  my_bigrams <- bidf %>%
    unnest_tokens(bigram, text, token = "ngrams", n=2) %>% 
    dplyr::count(bigram, sort = TRUE) %>%
    separate(bigram, c("word1", "word2"), sep = " ") %>%
    filter(!word1 %in% stop_words$word) %>%
    filter(!word2 %in% stop_words$word) %>%
    dplyr::count(word1, word2, sort = TRUE) %>%
    graph_from_data_frame() %>%
    ggraph(layout = "fr") +
    geom_edge_link(aes(edge_alpha = 100), show.legend = FALSE)+
    geom_node_point(color = "lightblue", size= 3 )+
    geom_node_text(aes(label=name), vjust =1, hjust=1, repel = TRUE)+#, repel=TRUE)
    theme_void()
}
